{"cells":[{"cell_type":"markdown","metadata":{"id":"xAfpx5kZOYMy"},"source":["# 交差検証とグリッドサーチ\n","## 前回行ったこと\n"," - `get_dummies`によるone-hotエンコーディング\n"," - 検証用データを用いたモデルの投稿前の評価"]},{"cell_type":"markdown","metadata":{"id":"W8gV4Z9yOYMz"},"source":["## 今回行うこと\n","- 交差検証とハイパーパラメータチューニングのためのクラス`GridSearchCV`の使い方\n","- 学習後のモデルの観察"]},{"cell_type":"markdown","metadata":{"id":"RXGUTv-POYM0"},"source":["## （復習）データの読み込み・特徴ベクトルの構築\n","前回の資料で構成した，one-hotエンコーディングを用いた特徴ベクトルを再び作ります．\n","もう詳しく説明することはしません．\n","詳細は前回の資料を参照してください．"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"F3kfu3uuOYM0","executionInfo":{"status":"ok","timestamp":1717152085087,"user_tz":-540,"elapsed":768,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_nA91lMOYM1","executionInfo":{"status":"ok","timestamp":1717152107237,"user_tz":-540,"elapsed":22171,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"595a2b11-782a-446f-e44b-96b984837764"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Google Colabを用いる場合、以下のコメントアウトを外して実行\n","# from google.colab import drive\n","# drive.mount('/content/drive') # google driveをマウント（＝Colabから使えるようにする）"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RjI8WVo2OYM1","executionInfo":{"status":"ok","timestamp":1717152148379,"user_tz":-540,"elapsed":2535,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}}},"outputs":[],"source":["# Google Colabを用いる場合、以下のコメントアウトを外して実行\n","# d_train = pd.read_csv(\"drive/My Drive/data/train.csv\") # 訓練データを読み込む．パスは適宜変更すること\n","# d_test = pd.read_csv(\"drive/My Drive/data/test.csv\") # テストデータを読み込む．パスは適宜変更すること"]},{"cell_type":"code","source":["# ローカル環境を用いる場合、以下のコメントアウトを外して実行\n","# d_train = pd.read_csv(\"/data/train.csv\") # 訓練データを読み込む．パスは適宜変更すること\n","# d_test = pd.read_csv(\"/data/test.csv\") # テストデータを読み込む．パスは適宜変更すること"],"metadata":{"id":"dla7G1d_PkOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XI7FQvJFOYM1","executionInfo":{"status":"ok","timestamp":1717152166721,"user_tz":-540,"elapsed":293,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"72725d1c-c0d9-47ea-fdd4-9b340cc305a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["訓練データとテストデータの数を取得\n","訓練データ数：79800，テストデータ数：34200\n","[43  0 50 ... 60 44 24]\n"]}],"source":["print(\"訓練データとテストデータの数を取得\")\n","n_train = len(d_train)\n","n_test = len(d_test)\n","print(f\"訓練データ数：{n_train}，テストデータ数：{n_test}\")\n","\n","# targetの値\n","y_train = d_train.pop('popularity')\n","y_train = y_train.to_numpy() # numpyのarrayに変換\n","print(y_train)"]},{"cell_type":"markdown","metadata":{"id":"XZ7_k7nLOYM2"},"source":["以下のセルでOne-hotエンコーディングを行い特徴ベクトルを作ります．\n","\n","変数に`_one_hot`とつけるのは面倒なので，今回は`X_train`，`X_test`という変数名にします．"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"O2R0kjlPOYM2","executionInfo":{"status":"ok","timestamp":1717152415331,"user_tz":-540,"elapsed":761,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}}},"outputs":[],"source":["d_all = pd.concat([d_train, d_test], axis=0) # 訓練データとテストデータを連結\n","d_all = d_all.astype({'explicit': int}) # `explicit`の列の型をintにキャスト\n","columns_cat = [\"track_genre\"] # カテゴリカル変数の列名\n","\n","d_all_onehot = pd.get_dummies(d_all, columns=columns_cat, dtype=int) # get_dummiesを使ってone-hotエンコーディング．columnsに指定された列のみone-hotエンコーディングし，出力としてint型を指定\n","d_train_onehot = d_all_onehot[:n_train] # d_all_onehotの訓練データ部分\n","d_test_onehot = d_all_onehot[n_train:] # d_all_onehotのテストデータ部分\n","X_train = d_train_onehot.select_dtypes(include=['int64', 'float64']).to_numpy() # 数値部分のみ取り出し，np.arrayに変換\n","X_test = d_test_onehot.select_dtypes(include=['int64', 'float64']).to_numpy()  # 数値部分のみ取り出し，np.arrayに変換"]},{"cell_type":"markdown","metadata":{"id":"_kL6nrMyOYM2"},"source":["いい加減しつこい気もしますが，復習と予測結果の比較のために`LinearRegression`を動かしておきます．\n","ハイパーパラメータ（学習するのではなく，ユーザが決める要素）はここでは`fit_intercept=False`とします．"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VCzYyH5OYM2","executionInfo":{"status":"ok","timestamp":1717152488144,"user_tz":-540,"elapsed":2781,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"90c1c081-387b-49f8-f8ea-f465fe88d330"},"outputs":[{"output_type":"stream","name":"stdout","text":["[36.07987563 32.40019485 55.05526121 ... 45.410423   42.58279019\n"," 40.51070228]\n"]}],"source":["from sklearn.linear_model import LinearRegression # LinearRegressionを使えるようにする\n","lr = LinearRegression(fit_intercept=False) # インスタンスの作成\n","lr.fit(X_train, y_train)\n","y_pred_test_lr = lr.predict(X_test)\n","print(y_pred_test_lr)"]},{"cell_type":"markdown","metadata":{"id":"KZaA1eZtOYM3"},"source":["## 交差検証（Cross Validation）\n","特徴ベクトルの作り方やどの手法を使うか，さらには同じ手法であっても**ハイパーパラメータ**（バイアス項を使うか？正則化項の強さはどれくらいか？中間層のユニット数はどれくらいか？等の，ユーザが定める要素）の値よって，予測の正確さは大きく変わってきます．\n","前回は「ラベル付きデータを訓練用/検証用に分割し，訓練用データで学習した後，学習に使わなかった検証用で評価」ということを行いました．\n","しかし，訓練・検証の分割の仕方によって評価の値は当然変わってきます．\n","分割によっては，本当は悪い手法であるのにもかかわらず良い評価となったり，また反対に，本当は良い手法であるのにもかかわらず悪い評価となることもあるでしょう．\n","\n","そこで，次のような上手い方法がしばしば行われます：データを`K`分割（`K=5,10`等）し，そのうちの`K-1`個の分割を訓練，`1`個の分割を検証データとして用いることを繰り返し，あるハイパーパラメータの時のモデルの性能を`K`回のスコア（評価値）の平均で評価する．\n","この方法を**（K-分割）交差検証（(K-fold) cross validation）**といいます．\n","K=訓練データ数のときは特にLeave-one-out交差検証（LOOCV）と呼ばれたりもします．\n","一回だけでなくK回の評価の平均を取っているので，前回の方法よりもより正確に性能の評価ができそうです．\n","一方で，学習をK回行うため，前回の方法と比べると時間はかかってしまいそうです．\n","\n","今回はsklearnの便利な関数を使って，いくつかの方法で交差検証を行ってみます．"]},{"cell_type":"markdown","metadata":{"id":"5u-WtcTgOYM3"},"source":["## Ridge回帰の交差検証+グリッドサーチによるハイパーパラメータチューニング\n","### Ridge回帰：正則化付きの線形回帰\n","今，特徴ベクトルと使う手法は固定して，その手法の適切なハイパーパラメータを定めるということを考えます．\n","前回までの例でいうと，特徴ベクトルは数値情報+one-hotエンコーディング，手法は`LinearRegression`と決めて，`LinearRegression`においてバイアス項を使うか否か（`fit_intercept`を`True`にするか`False`にするか）を定めることに対応します．\n","\n","ずっと線形回帰を使うのも楽しくありませんが，かと言っていきなり複雑な手法を使うのもすこし大変ですので，今回は線形回帰に**正則化**項を加えたRidge回帰，sklearnにおける`Ridge`を用いることにします．\n","正則化は過学習を防ぐための方法の一つで，Ridge回帰は線形回帰の目的関数にパラメータの $L_2$ ノルムの2乗を加えたものでした（講義データサイエンスの初回の資料参照）．\n","線形モデルにおいて，重みの値が非常に大きい場合，入力の値がほんの少し変化しただけで予測の値が大きく変わってしまいます．\n","そのような場合，訓練データに対しては正しい予測ができるけれども，訓練データとほんの少し値が違うだけのデータに対しては正しい予測ができなくなってしまいます．\n","これはまさしく過学習です．\n","しかし，重みの値に対して罰則を課す正則化項を目的関数に加えることで，重みの値を過剰に大きくしてまでは訓練データにフィットしなくなり，過学習を防げます．\n","\n","Ridge回帰は以下のようにしてimportできます．"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"itF6ec01OYM3","executionInfo":{"status":"ok","timestamp":1717152615081,"user_tz":-540,"elapsed":301,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}}},"outputs":[],"source":["from sklearn.linear_model import Ridge"]},{"cell_type":"markdown","metadata":{"id":"GQRPRxbLOYM3"},"source":["[Ridgeのドキュメント](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)を見てみましょう．\n","`LinearRegression`と比べるとユーザが定めることのできる要素が色々と追加されています．\n","`fit_intercept`は`LinearRegression`にもありました．\n","`alpha`がここでは最も重要で，正則化項の強さを決めるハイパーパラメータです．\n","`Ridge`回帰においてパラメータを定める方法（学習アルゴリズム）はいくつかあり，それは`solver`引数で指定することができます．\n","`max_iter`や`tol`，`random_state`は`solver`に関係する引数です．\n","本来はデータの数や次元数などを考慮して適切に選ぶべき（学習時間に大きく影響します）ですが一旦置いておきます．ここではとりあえず，`LinearRegression`との比較のためにsolver=\"svd\"`とだけ定めます．\n","\n","練習がてら，`alpha=10.0`，`fit_intercept=False`で動かしてみましょう．"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhTvNOT0OYM3","executionInfo":{"status":"ok","timestamp":1717152859387,"user_tz":-540,"elapsed":2804,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"0c96b37d-dd76-4faa-82fe-e508c8b5d034"},"outputs":[{"output_type":"stream","name":"stdout","text":["[36.39355551 32.51309548 53.76571668 ... 46.47130878 44.77646633\n"," 39.2655409 ]\n"]}],"source":["ridge = Ridge(alpha=10.0, fit_intercept=False, solver=\"svd\")\n","ridge.fit(X_train, y_train)\n","y_pred_test_ridge = ridge.predict(X_test)\n","print(y_pred_test_ridge)"]},{"cell_type":"markdown","metadata":{"id":"5L5UW8-BOYM3"},"source":["上の`LinearRegression`の結果と比べると，やや異なる結果となっています．\n","正則化が正しく動いていれば，パラメータ $\\mathbf{w}$ のノルムが小さくなっているはずです．\n","ちょっと本題とは話がそれてしまいますが，ついでにこのことを確認してみましょう．\n","\n","Pythonでは，あるオブジェクト`obj`の持つ`a`というフィールドには，`obj.a`という形式でアクセスできます．\n","`Ridge`や`LinearRegression`のドキュメントを見てみると，重み $\\mathbf{w}$ には`coef_`という名前がついていることが分かります．\n"]},{"cell_type":"markdown","source":["   $L_2$ ノルムの計算は自分でコードを書いてもよいですが，numpyにはちゃんと（？）ノルムを計算する関数[`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)があるので使ってみましょう．`numpy.linalg.norm`では`ord`(order)パラメータによってノルムの種類を指定しますが，何も指定しないとベクトルの入力に対して $L_2$ ノルムを計算してくれるのでこれを使います．"],"metadata":{"id":"Q8XGMVb5TwB4"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_FuAZCHOYM4","executionInfo":{"status":"ok","timestamp":1717153423885,"user_tz":-540,"elapsed":350,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"06a1bcf2-9b03-4622-d468-4a6d066aea82"},"outputs":[{"output_type":"stream","name":"stdout","text":["線形回帰の重みのノルム：377.6741303081109\n","Ridge回帰の重みのノルム：149.95095135145374\n"]}],"source":["norm2_lr = np.linalg.norm(lr.coef_)\n","norm2_ridge = np.linalg.norm(ridge.coef_)\n","\n","print(f\"線形回帰の重みのノルム：{norm2_lr}\")\n","print(f\"Ridge回帰の重みのノルム：{norm2_ridge}\")"]},{"cell_type":"markdown","metadata":{"id":"838kLbNOOYM4"},"source":["結果が出てきました！\n","線形回帰の重みのノルムと比べると，Ridge回帰の重みのノルムは小さくなっていて，正則化が働いていることがわかります．"]},{"cell_type":"markdown","metadata":{"id":"OVW2fVIqOYM4"},"source":["### Ridge回帰のハイパーパラメータチューニング：`alpha`のみチューニング\n","\n","より大きな`alpha`を設定すると正則化の効果が強くなり，より重みのノルムが小さくなります（演習では行いませんが，実験ではこのことを実際に確認するはずです）．\n","`alpha`を非常に大きな値にした場合，重みのノルムはほとんど`0`に近い値となり，訓練データに対する性能もテストデータに対する性能も差はないがどちらも非常に低い，アンダーフィッティング（under fitting）が起こってしまいます．\n","一方で，`alpha`の値が小さすぎると正則化の効果がほとんどなくなってしまいます．\n","そのため，`alpha`は大きすぎても小さすぎてもだめで，イイカンジの値を定める必要があります．\n","\n","ようやく本題です．そこで，**交差検証によって適切な`alpha`の値を定める**ことにします．\n","`alpha`は理論上は任意の非負の値をとることができますが，コンピュータ上ではそれは不可能です．\n","一般に，連続値を取るハイパーパラメータは，適当に上限と下限と間隔を定め，その中で良い値を探します．\n","例えば，`[0.1, 0.2, 0.3, 0.4, 0.5]`であったり，`[1e-3, 1e-2, 1e-1, 1, 10, 100]`と言ったようにです．\n","とりあえず今回は`[1e-3, 1e-2, 1e-1, 1, 10, 100]`の範囲で探してみましょう．\n","また，一旦`fit_intercept`は先程と同じように`False`としておきます．\n","\n","さらに，交差検証を行う場合，何分割するかを考える必要があります．\n","とりあえずここでは分割数`K=5`としておきましょう．\n"]},{"cell_type":"markdown","metadata":{"id":"9SKKvkxiOYM4"},"source":["これにて行うことが定まりました．以下のようになりそうです：\n","  1. 訓練データを5分割し，\n","  2. ある分割を検証用，その他を訓練用として，`alpha`をある範囲内（ここでは`[1e-3, 1e-2, 1e-1, 1, 10, 100]`）で動かして学習・評価することを繰り返し，\n","  3. ある範囲内の全ての値で学習・評価が終わった後，その結果を元に最も良い`alpha`を定める．\n","\n","では残りはコードを書くだけなのですが，それなりに面倒なように感じます（もちろん，「書け」と言われれば書けるとは思いますが）．\n","勉強のために自分で書くことは非常に大事ですが（実験では皆さんに書いてもらうはずです），ここでは偉大なるsklearnの便利な機能を利用します．\n","sklearnには，[cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)という関数があり，これを使うと簡単に交差検証ができます．\n","`cross_validate`関数を使う場合，以下の引数を指定する必要があります：\n","- `estimator`：交差検証によって評価をしたいインスタンス（例：`ridge`）\n","- `X`：訓練データ（例：`X_train`）\n","- `y`：訓練データの目標値（例：`y_train`）\n","- `cv`：分割数，整数型（例：5)\n","- `scoring`：評価関数，文字列（例：`\"neg_mean_squared_error\"`）\n","\n","そして以下の情報（キー）を持った**辞書型オブジェクト（=ハッシュテーブル）**を返します：\n","- `fit_time`：学習にかかった時間のリスト（要素数K)\n","- `score_time`：評価にかかった時間のリスト（要素数K)\n","- `test_score`：検証用データでのスコアのリスト（要素数K）\n","\n","詳しくは上のドキュメントを読んでみてください（実際はここで述べているよりも更に柔軟です）．\n","今回のコンペにおける評価関数は平均二乗誤差でした．sklearnには`mean_squared_error`という関数があるためこれを`scoring`に使いたいのですが，残念ながら対応していないため代わりに`neg_mean_squared_error`を使います．\n","これは負の平均二乗誤差です（\"neg\"は\"negative\"）．\n","\n","とりあえず使ってみましょう．\n","`alpha=10.0`の`ridge`を検証してみます．"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_b3BQBKWOYM4","executionInfo":{"status":"ok","timestamp":1717154027748,"user_tz":-540,"elapsed":14792,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"02d29b2d-d805-497f-9f88-e44bee378c2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'fit_time': array([3.8787272 , 3.69170451, 3.09351087, 1.86862659, 1.90792036]), 'score_time': array([0.01690435, 0.01775718, 0.00661945, 0.00646496, 0.00445151]), 'test_score': array([-370.43438561, -376.62073497, -367.48155642, -376.39055146,\n","       -376.14835632])}\n"]}],"source":["from sklearn.model_selection import cross_validate\n","ridge = Ridge(alpha=10.0, fit_intercept=False, solver=\"svd\")\n","scores = cross_validate(ridge, X_train, y_train, cv=5,\n","                        scoring=\"neg_mean_squared_error\")\n","print(scores)"]},{"cell_type":"markdown","metadata":{"id":"wF6tUo1POYM4"},"source":["色々`print`されました．\n","ちょっと見にくいですね．\n","今は検証用データのスコアさえあれば良いので，`\"test_score\"`だけ取り出します．\n","また，5回の評価結果の平均を計算します．\n","`np.mean`によってリストや`np.array`に含まれる値の平均を計算できます．\n","また，符号が反転しているので，わかりやすさのために戻しておきましょう．"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3fBOpNcOYM5","executionInfo":{"status":"ok","timestamp":1717154053143,"user_tz":-540,"elapsed":309,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"bb8b9c87-211f-42e2-c433-9ffb89807b6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["交差検証の5回のスコア：[370.43438561 376.62073497 367.48155642 376.39055146 376.14835632]\n","交差検証の平均スコア：373.4151169555022\n"]}],"source":["print(f\"交差検証の5回のスコア：{-scores['test_score']}\")\n","print(f\"交差検証の平均スコア：{-np.mean(scores['test_score'])}\")"]},{"cell_type":"markdown","metadata":{"id":"hWFIXyECOYM5"},"source":["`alpha=10.0`の場合の結果が無事に出てきました．"]},{"cell_type":"markdown","metadata":{"id":"AcBCX9nCOYM5"},"source":["次は，`alpha`を`[1e-3, 1e-2, 1e-1, 1, 10, 100]`の範囲で動かして5分割交差検証を行い，その結果において最も良い`alpha`とその交差検証のスコア（5回の検証スコアの平均）を探索してみます．\n","ここで，`alpha`の探索範囲のリストとして`alpha_list`を定義しておきます．\n","`results`は，それぞれの`alpha`における交差検証のスコアを格納するリストです．"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meakgV1BOYM5","executionInfo":{"status":"ok","timestamp":1717154552168,"user_tz":-540,"elapsed":66273,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"27f20b4a-eeac-4d25-febc-58af0724c854"},"outputs":[{"output_type":"stream","name":"stdout","text":["最良のalpha：0.001　交差検証スコア：369.94445475190776\n"]}],"source":["alpha_list = [1e-3, 1e-2, 1e-1, 1, 10, 100] # 探索するalphaの範囲\n","results = []\n","for alpha in alpha_list:\n","    ridge = Ridge(alpha=alpha, fit_intercept=False, solver=\"svd\")\n","    scores = cross_validate(ridge, X_train, y_train, cv=5,\n","                            scoring=\"neg_mean_squared_error\")\n","    results.append(-np.mean(scores[\"test_score\"])) # resultsに結果を追加\n","\n","result_best = np.min(results) # 最善のスコア\n","alpha_best = alpha_list[np.argmin(results)] # 最善のスコアを出した時のalphaの値(numpy.argminは最小値のインデックスを取得)\n","print(f\"最良のalpha：{alpha_best}　交差検証スコア：{result_best}\")"]},{"cell_type":"markdown","metadata":{"id":"v2AluRRbOYM5"},"source":["結果が出てきたでしょうか？\n","私の環境では，`alpha=0.001`が（この範囲では）最もよく，またその時のスコアは`369.94445475190776`となっています．"]},{"cell_type":"markdown","metadata":{"id":"vEwK4YTuOYM5"},"source":["### Ridge回帰のハイパーパラメータチューニング：`alpha`と`fit_intercept`のチューニング\n","\n","今度は`alpha`だけでなく`fit_intercept`もチューニングしてみましょう．\n","チューニングするハイパーパラメータが複数ある場合によく行われる方法は，全てのパラメータについて探索範囲を定め，その範囲の全ての組合せを試して（=範囲の直積上で探索して），その中で最も良いハイパーパラメータの組合せを探す，というものです．\n","これを**グリッドサーチ**といいます．\n","例えば，`alpha`を`[1, 10, 100]`，`fit_intercept`を`[True, False]`の範囲で探索する場合，\n"," - `alpha=1`，`fit_intercept=True`\n"," - `alpha=1`，`fit_intercept=False`\n"," - `alpha=10`，`fit_intercept=True`\n"," - `alpha=10`，`fit_intercept=False`\n"," - `alpha=100`，`fit_intercept=True`\n"," - `alpha=100`，`fit_intercept=False`\n","\n","の合計 $3\\times2=6$ 通りを試します．\n","\n","\n","では実際に行ってみようと思います……が，はっきり言ってやや面倒に感じます．\n","なんと素晴らしいことに，sklearnにはこれを簡単に行う`GridSearchCV`というクラスが提供されています．\n","`GridSearchCV`は先程の`cross_validate`（関数）とは異なりクラスです．\n","`GridSearchCV`のインスタンスを作成する際に，ハイパーパラメータチューニングを行いたいモデルとハイパーパラメータの探索範囲を与えます．\n","そして，作成した`GridSearchCV`のインスタンスの`fit`メソッドを実行すると，交差検証を行い最も良いハイパーパラメータを見つけてくれます．\n","`GridSearchCV`のインスタンスを作成する際に，最低限指定しなければならない引数を説明します．\n"," - `estimator`：探索・評価をしたいモデル（例：`ridge`）\n"," - `param_grid`：探索を行うハイパーパラメータとその範囲，辞書型（keyがハイパーパラメータの名前（文字列），valueが探索範囲（リスト））．\n"," - `scoring`：評価関数，文字列（例：\"neg_mean_squared_error\"）\n"," - `cv`：分割の数，整数．\n","\n","詳しくは[GridSearchCVのドキュメント](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)参照してください．\n","これもやはり実際は更に柔軟です．\n","注意が必要なのが，「最も良い」の意味です．\n","`GridSearchCV`では，`scoring`は「モデル（予測結果）の良さ」を表す関数であるとしています．\n","つまり，**スコアの最も高い**ハイパーパラメータを返します．\n","`GridSearchCV`では，`scoring`に文字列ではなく自身の作成した関数を渡すこともできますが，うっかり二乗誤差のように「モデル（予測結果）の悪さ」を表す関数を渡すと最も悪いハイパーパラメータが返ってきます．\n","`scoring`に`\"mean_squared_error\"`が指定できず，`\"neg_mean_squared_error\"`が用意されている理由はこの仕様のためです．\n","\n","文章で説明してもわかりにくいと思いますので，とりあえず`fit_intercept`の探索範囲を`[True, False]`，`alpha`の探索範囲を先ほどと同じ範囲として動かしてみます．"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"QY68-hPhOYM5","executionInfo":{"status":"ok","timestamp":1717154928257,"user_tz":-540,"elapsed":152193,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"95f9e8cb-9eef-4491-b8cf-32bebdd1577d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, estimator=Ridge(solver='svd'),\n","             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n","                         'fit_intercept': [True, False]},\n","             scoring='neg_mean_squared_error')"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Ridge(solver=&#x27;svd&#x27;),\n","             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n","                         &#x27;fit_intercept&#x27;: [True, False]},\n","             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Ridge(solver=&#x27;svd&#x27;),\n","             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n","                         &#x27;fit_intercept&#x27;: [True, False]},\n","             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(solver=&#x27;svd&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(solver=&#x27;svd&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"]},"metadata":{},"execution_count":14}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","estimator = Ridge(solver=\"svd\") # 探索するモデルを決める\n","param_grid = { # 探索するハイパーパラメータと範囲を決める\n","    \"alpha\": alpha_list,\n","    \"fit_intercept\": [True, False]\n","} # solverやmax_iterはここでは指定されていないので，ずっと同じ値\n","\n","cv = GridSearchCV(estimator=estimator, # estimator引数でどのモデルを使うかを指定\n","                  param_grid=param_grid, # param_grid引数で探索するハイパーパラメータと範囲を指定\n","                  scoring='neg_mean_squared_error', cv=5) # scoringで評価指標を，cvで分割数を指定\n","cv.fit(X_train, y_train) # 交差検証を実行"]},{"cell_type":"markdown","metadata":{"id":"3eBWJepBOYM6"},"source":["交差検証が終了すると，当然，その結果が知りたいです．\n","`fit`実行後の`GridSeachCV`は，\n","1. 交差検証による評価で最も良かったハイパーパラメータ：`best_params_`\n","2. 最も良かったハイパーパラメータを用いたときのスコア：`best_score_`\n","3. 最も良かったハイパーパラメータを用いたときのモデル：`best_estimator_`\n","\n","を持っています．\n","最も良かったハイパーパラメータとその交差検証スコアを確認し，さらに`best_estimator_`で予測した結果を保存してみましょう．"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0eLZhEPOYM6","executionInfo":{"status":"ok","timestamp":1717154952501,"user_tz":-540,"elapsed":1075,"user":{"displayName":"野原大靖","userId":"01964219192621438108"}},"outputId":"25bf3d69-5511-4748-bf8f-bb4cfe96ef61"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'alpha': 1, 'fit_intercept': True}\n","369.9420668019845\n","[36.08183409 32.40004005 55.02918037 ... 45.39194558 42.56519461\n"," 40.49837409]\n"]}],"source":["print(cv.best_params_) # 最も良かったハイパーパラメータを見てみる\n","print(-cv.best_score_) # 最も良かったハイパーパラメータの時のスコアを見てみる．わかりやすさのため符号は反転する．\n","y_pred_ridge_gridcv = cv.best_estimator_.predict(X_test)# 最も良かったモデルで推定する．実はcv.predictでも良い\n","np.savetxt(X=y_pred_ridge_gridcv, fname='y_pred_ridge_cv.csv')\n","print(y_pred_ridge_gridcv)"]},{"cell_type":"markdown","metadata":{"id":"gExT07LHOYM6"},"source":["結果が出てきました．\n","上の探索範囲ではこのモデルが最も良いと考えられるので，この結果を投稿してみると良いかもしれません（しなくても良いです）．"]},{"cell_type":"markdown","metadata":{"id":"K5KR7_oNOYM6"},"source":["## まとめ\n","- K分割交差検証：ラベル付きデータをK分割し，K-1個で学習，残りの一個で評価をK回繰り返し，手法を評価する．\n","- グリッドサーチ：それぞれのハイパーパラメータの探索範囲を定めて，全ての組合せを試して良いハイパーパラメータを決める．\n","- `GridSearchCV`を使うとハイパーパラメータチューニングが簡単にできる．"]},{"cell_type":"markdown","metadata":{"id":"C9OAOTtBOYM6"},"source":["今回はRidge回帰を取り上げましたが，予測手法は他にも多数あります．\n","Ridge回帰や線型回帰は**非常に単純なモデル**で，まず試すべきものの一つかとは思いますが，非線形でより複雑なモデルの方が良い性能であることが多々あります．\n","他の手法であっても，`fit`で学習し`predict`で予測することは変わらないですし，`GridSearchCV`の使い方も同じです（勿論，ハイパーパラメータの名前や有効な範囲がわかってないといけないため，ドキュメントを読む必要は間違いなくありますが）．\n","是非いろいろ試してみてください．\n","また，sklearn以外にも機械学習ライブラリはありますので，興味がある方は調べてみると良いかもしれません．\n","sklearnには様々な手法が実装されていますが，特定の手法に特化したライブラリが他にいくつもあります．\n","\n","グリッドサーチではハイパーパラメータの候補点をそれぞれの探索範囲の直積から取ってきましたが，候補点をランダムに決める（＝グリッドになっていない）ランダムサーチ（`RandomizedSearchCV`）であったり（少数のハイパーパラメータの影響が強い場合に有効），ハイパーパラメータに関しても最適化する（＝もっと賢くハイパーパラメータを探す）方法も提案されています．\n","後者についてはsklearnにはありませんが，他にライブラリがあるので，興味がある方は調べてみると良いかもしれません（一回の学習が重たい手法では，少ない学習回数で良いハイパーパラメータを見つけたいため，ハイパーパラメータチューニングの手法が重要になってきます）．\n","もっとレイヤーが上がったところだと，「どの手法を使うか」「どのような特徴ベクトルを作るか」というところも上手くやる，**Automated Machine Learning（AutoML）**というものもあり，AutoMLのライブラリもあったりします．\n","興味がある方はこちらも調べてみると面白いかもしれません．"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}